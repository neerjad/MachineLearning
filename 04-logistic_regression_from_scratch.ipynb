{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from scratch\n",
    "Gradient descent approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [4, 5], [2, 5]])\n",
    "y = np.array([1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[3, 1], [3, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "        Computes the sigmoid\n",
    "        '''\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def loss_function(self):\n",
    "        '''\n",
    "        Computes log loss for binary classification. \n",
    "        In case ofmulti class classigication, we need to compute cross entropy\n",
    "        '''\n",
    "        loss = (-self.y * np.log(self.train_pred + 0.0001) - (1-self.y)*np.log(1-self.train_pred + 0.0001)).mean()\n",
    "        return loss\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        '''\n",
    "        Updates the weights by computing: W - learning rate * gradient.\n",
    "        gradient = x * (sigmoid(z) - y)\n",
    "        \n",
    "        Returns:\n",
    "        W - Updates weights\n",
    "        '''\n",
    "        grad = np.dot(self.X.T, (self.train_pred - self.y)) * self.learning_rate\n",
    "        self.W = self.W - grad\n",
    "        return self.W\n",
    "    \n",
    "    def log_reg(self, learning_rate, iterations):\n",
    "        '''\n",
    "        Function that iterates over \"iterations\" to compute loss and then \n",
    "        performs gradient descent to update the weights\n",
    "        \n",
    "        Inputs:\n",
    "        learning_rate - learning rate\n",
    "        iterations - Number of iteations\n",
    "        \n",
    "        ''' \n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.randn(len(self.X[0])) # weights are randomly initialised at first\n",
    "        for it in range(iterations):\n",
    "            self.train_pred = []\n",
    "            for row in self.X:\n",
    "                z = np.sum([row[i] * self.W[i] for i in range(len(row))])\n",
    "                self.train_pred.append(self.sigmoid(z))\n",
    "            self.train_pred = np.array(self.train_pred)\n",
    "            loss = self.loss_function()\n",
    "            print(f'Epoch {it}, loss {loss}')\n",
    "            self.W = self.gradient_descent()\n",
    "        print(f'Coefficients: {self.W}')\n",
    "        \n",
    "    def predict(self, test):\n",
    "        '''\n",
    "        Multiply the features of every row with the corresponding coefficients\n",
    "        '''\n",
    "        self.test_pred = []\n",
    "        for row in test:\n",
    "            z = np.sum([row[i] * self.W[i] for i in range(len(row))])\n",
    "            self.test_pred.append(self.sigmoid(z))\n",
    "        print(f'Predictions: {self.test_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit logistic regression to get the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 1.1350142379847765\n",
      "Epoch 1, loss 0.5303797437020138\n",
      "Epoch 2, loss 0.4796497048366155\n",
      "Epoch 3, loss 0.46323724890285595\n",
      "Epoch 4, loss 0.4609230721947349\n",
      "Epoch 5, loss 0.4607776617596388\n",
      "Epoch 6, loss 0.46076331529137554\n",
      "Epoch 7, loss 0.4607538648441638\n",
      "Epoch 8, loss 0.4607448184849872\n",
      "Epoch 9, loss 0.4607360427770874\n",
      "Coefficients: [-0.53004575 -0.07845485]\n"
     ]
    }
   ],
   "source": [
    "model.log_reg(learning_rate=0.1, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the log loss decreases with every iteration\n",
    "\n",
    "### Predict target values for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0.15861197983194045, 0.12106200618100489]\n"
     ]
    }
   ],
   "source": [
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
